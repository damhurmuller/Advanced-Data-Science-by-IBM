{"cells": [{"metadata": {}, "cell_type": "code", "source": "!rm -f hmp.parquet*", "execution_count": 1, "outputs": [{"output_type": "stream", "text": "Waiting for a Spark session to start...\nSpark Initialization Done! ApplicationId = app-20200116040724-0000\nKERNEL_ID = 705fbdbe-b142-4466-9658-4ac65ea634fd\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "!wget https://github.com/IBM/coursera/raw/master/hmp.parquet", "execution_count": 2, "outputs": [{"output_type": "stream", "text": "--2020-01-16 04:07:28--  https://github.com/IBM/coursera/raw/master/hmp.parquet\nResolving github.com (github.com)... 140.82.113.3\nConnecting to github.com (github.com)|140.82.113.3|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://raw.githubusercontent.com/IBM/coursera/master/hmp.parquet [following]\n--2020-01-16 04:07:28--  https://raw.githubusercontent.com/IBM/coursera/master/hmp.parquet\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 199.232.8.133\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|199.232.8.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 932997 (911K) [application/octet-stream]\nSaving to: 'hmp.parquet'\n\n100%[======================================>] 932,997     --.-K/s   in 0.03s   \n\n2020-01-16 04:07:29 (30.8 MB/s) - 'hmp.parquet' saved [932997/932997]\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "df = spark.read.parquet('hmp.parquet')\ndf.createOrReplaceTempView('df')\ndf.show()", "execution_count": 3, "outputs": [{"output_type": "stream", "text": "+---+---+---+--------------------+-----------+\n|  x|  y|  z|              source|      class|\n+---+---+---+--------------------+-----------+\n| 22| 49| 35|Accelerometer-201...|Brush_teeth|\n| 22| 49| 35|Accelerometer-201...|Brush_teeth|\n| 22| 52| 35|Accelerometer-201...|Brush_teeth|\n| 22| 52| 35|Accelerometer-201...|Brush_teeth|\n| 21| 52| 34|Accelerometer-201...|Brush_teeth|\n| 22| 51| 34|Accelerometer-201...|Brush_teeth|\n| 20| 50| 35|Accelerometer-201...|Brush_teeth|\n| 22| 52| 34|Accelerometer-201...|Brush_teeth|\n| 22| 50| 34|Accelerometer-201...|Brush_teeth|\n| 22| 51| 35|Accelerometer-201...|Brush_teeth|\n| 21| 51| 33|Accelerometer-201...|Brush_teeth|\n| 20| 50| 34|Accelerometer-201...|Brush_teeth|\n| 21| 49| 33|Accelerometer-201...|Brush_teeth|\n| 21| 49| 33|Accelerometer-201...|Brush_teeth|\n| 20| 51| 35|Accelerometer-201...|Brush_teeth|\n| 18| 49| 34|Accelerometer-201...|Brush_teeth|\n| 19| 48| 34|Accelerometer-201...|Brush_teeth|\n| 16| 53| 34|Accelerometer-201...|Brush_teeth|\n| 18| 52| 35|Accelerometer-201...|Brush_teeth|\n| 18| 51| 32|Accelerometer-201...|Brush_teeth|\n+---+---+---+--------------------+-----------+\nonly showing top 20 rows\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "splits = df.randomSplit([0.8, 0.2])\ndf_train = splits[0]\ndf_test = splits[1]", "execution_count": 4, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "from pyspark.ml.feature import StringIndexer\nfrom pyspark.ml.feature import OneHotEncoder\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.feature import Normalizer\n\nindexer = StringIndexer(inputCol=\"class\", outputCol=\"label\")\nencoder = OneHotEncoder(inputCol=\"label\", outputCol=\"labelVec\")\nvectorAssembler = VectorAssembler(inputCols=[\"x\",\"y\",\"z\"], outputCol=\"features\")\nnormalizer = Normalizer(inputCol=\"features\", outputCol=\"features_norm\", p=1.0)", "execution_count": 8, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "from pyspark.ml.classification import LinearSVC\n\nlsvc = LinearSVC(maxIter=10, regParam=0.1)", "execution_count": 10, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "df.createOrReplaceTempView('df')\ndf_two_class = spark.sql(\"\"\"\n    select * from df where class in ('Use_telephone','Standup_chair')\n\"\"\")", "execution_count": 17, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "splits = df_two_class.randomSplit([0.8, 0.2])\ndf_train = splits[0]\ndf_test = splits[1]", "execution_count": 18, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "df_train.show()", "execution_count": 19, "outputs": [{"output_type": "stream", "text": "+---+---+---+--------------------+-------------+\n|  x|  y|  z|              source|        class|\n+---+---+---+--------------------+-------------+\n|  0| 30| 24|Accelerometer-201...|Standup_chair|\n|  0| 31| 30|Accelerometer-201...|Standup_chair|\n|  0| 31| 32|Accelerometer-201...|Standup_chair|\n|  0| 31| 32|Accelerometer-201...|Standup_chair|\n|  0| 35| 28|Accelerometer-201...|Standup_chair|\n|  0| 37| 26|Accelerometer-201...|Standup_chair|\n|  0| 37| 35|Accelerometer-201...|Standup_chair|\n|  0| 38| 37|Accelerometer-201...|Standup_chair|\n|  0| 40| 34|Accelerometer-201...|Standup_chair|\n|  0| 42| 34|Accelerometer-201...|Standup_chair|\n|  0| 43| 34|Accelerometer-201...|Standup_chair|\n|  0| 43| 34|Accelerometer-201...|Standup_chair|\n|  0| 44| 21|Accelerometer-201...|Standup_chair|\n|  0| 56| 30|Accelerometer-201...|Standup_chair|\n|  1| 25| 29|Accelerometer-201...|Standup_chair|\n|  1| 27| 20|Accelerometer-201...|Standup_chair|\n|  1| 29| 23|Accelerometer-201...|Standup_chair|\n|  1| 29| 27|Accelerometer-201...|Standup_chair|\n|  1| 34| 30|Accelerometer-201...|Standup_chair|\n|  1| 34| 30|Accelerometer-201...|Standup_chair|\n+---+---+---+--------------------+-------------+\nonly showing top 20 rows\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "from pyspark.ml import Pipeline\n\npipeline = Pipeline(stages=[indexer, encoder, vectorAssembler, normalizer, lsvc])", "execution_count": 20, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "model = pipeline.fit(df_train)", "execution_count": 21, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "prediction = model.transform(df_train)", "execution_count": 22, "outputs": [{"output_type": "stream", "text": "Exception ignored in: <object repr() failed>\nTraceback (most recent call last):\n  File \"/opt/ibm/spark/python/pyspark/ml/wrapper.py\", line 105, in __del__\n    SparkContext._active_spark_context._gateway.detach(self._java_obj)\nAttributeError: 'LinearSVC' object has no attribute '_java_obj'\n", "name": "stderr"}]}, {"metadata": {}, "cell_type": "code", "source": "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n\nevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\nevaluator.evaluate(prediction)", "execution_count": 23, "outputs": [{"output_type": "execute_result", "execution_count": 23, "data": {"text/plain": "0.9392041026607255"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "prediction = model.transform(df_test)", "execution_count": 24, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\nevaluator.evaluate(prediction)", "execution_count": 25, "outputs": [{"output_type": "execute_result", "execution_count": 25, "data": {"text/plain": "0.9371816338170507"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python36", "display_name": "Python 3.6 with Spark", "language": "python3"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "name": "python", "pygments_lexer": "ipython3", "version": "3.6.8", "file_extension": ".py", "codemirror_mode": {"version": 3, "name": "ipython"}}}, "nbformat": 4, "nbformat_minor": 1}